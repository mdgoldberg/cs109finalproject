{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reupload all data to have as one dataset again\n",
    "completedf1 = pd.concat([pd.read_csv('0208plays.csv'),pd.read_csv('0914plays.csv')])\n",
    "\n",
    "#(remove 2002, because it has served it's purpose of creating 2003 priors)\n",
    "completedf = completedf1[completedf1['year']>2002].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print completedf.shape\n",
    "completedf.head(2)\n",
    "\n",
    "lcols = ['distToGoal',\"inDown3\",\"inDown2\",\"inDown1\",'inQuarter4',\"inQuarter3\",\"inQuarter2\",\"inQuarter1\",'secsElapsedInHalf',\"margin\",\"yds_to_go\",\"addscore\",\"tm_TO_left\",\"opp_TO_left\",\"inFGRange\", \"inRedZone\", \"tm_winprob\",\"inLast3minHalf\", \"tm_prev_yr_pass\",\"tm_in_season_pass\",\"tm_in_game_pass\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def masking(df,itrain,itest):    \n",
    "    mask=np.ones(df.shape[0], dtype='int')\n",
    "    mask[itrain]=1\n",
    "    mask[itest]=0\n",
    "    mask = (mask==1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runsvm(Xmatrix_train, Yresp_train, Cs):\n",
    "    from sklearn.svm import LinearSVC\n",
    "    clfsvm_lin=LinearSVC(loss=\"hinge\")\n",
    "    \n",
    "    from sklearn.grid_search import GridSearchCV\n",
    "    gs=GridSearchCV(clfsvm_lin, param_grid={'C':Cs}, cv=5)\n",
    "    gs.fit(Xmatrix_train, Yresp_train)\n",
    "    print \"BEST\", gs.best_params_, gs.best_score_, gs.grid_scores_\n",
    "    \n",
    "    best = gs.best_estimator_\n",
    "    print best.fit(Xmatrix_train, Yresp_train)\n",
    "    print best.score(Xmatrix_test, Yresp_test)\n",
    "    \n",
    "    return best\n",
    "    \n",
    "\n",
    "def runsvc(Xmatrix_train, Yresp_train, Cs):\n",
    "    from sklearn.svm import SVC\n",
    "    clfsvm_orig = SVC()\n",
    "    gs_orig = GridSearchCV(clfsvm_orig, param_grid={'C':Cs}, cv=5)\n",
    "    gs_orig.fit(Xmatrix_train, Yresp_train)\n",
    "    print \"BEST\", gs_orig.best_params_, gs_orig.best_score_, gs_orig.grid_scores_\n",
    "    \n",
    "    best_orig = gs_orig.best_estimator_\n",
    "    print best_orig.fit(Xmatrix_train, Yresp_train)\n",
    "    print best_orig.score(Xmatrix_test, Yresp_test)\n",
    "    \n",
    "    return best_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cv_optimize(clf, parameters, X, y, n_folds=5, score_func=None):\n",
    "    if score_func:\n",
    "        gs = GridSearchCV(clf, param_grid=parameters, cv=n_folds, scoring=score_func)\n",
    "    else:\n",
    "        gs = GridSearchCV(clf, param_grid=parameters, cv=n_folds)\n",
    "    gs.fit(X, y)\n",
    "    print \"BEST\", gs.best_params_, gs.best_score_, gs.grid_scores_\n",
    "    best = gs.best_estimator_\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def do_classify(clf, parameters, indf, featurenames, targetname, target1val, mask=None, reuse_split=None, score_func=None, n_folds=5):\n",
    "    subdf=indf[featurenames]\n",
    "    X=subdf.values\n",
    "    y=(indf[targetname].values==target1val)*1\n",
    "    if mask !=None:\n",
    "        print \"using mask\"\n",
    "        Xtrain, Xtest, ytrain, ytest = X[mask], X[~mask], y[mask], y[~mask]\n",
    "    if reuse_split !=None:\n",
    "        print \"using reuse split\"\n",
    "        Xtrain, Xtest, ytrain, ytest = reuse_split['Xtrain'], reuse_split['Xtest'], reuse_split['ytrain'], reuse_split['ytest']\n",
    "    if parameters:\n",
    "        clf = cv_optimize(clf, parameters, Xtrain, ytrain, n_folds=n_folds, score_func=score_func)\n",
    "    clf=clf.fit(Xtrain, ytrain)\n",
    "    training_accuracy = clf.score(Xtrain, ytrain)\n",
    "    test_accuracy = clf.score(Xtest, ytest)\n",
    "    print \"############# based on standard predict ################\"\n",
    "    print \"Accuracy on training data: %0.6f\" % (training_accuracy)\n",
    "    print \"Accuracy on test data:     %0.6f\" % (test_accuracy)\n",
    "    print confusion_matrix(ytest, clf.predict(Xtest))\n",
    "    print \"########################################################\"\n",
    "    return clf, Xtrain, ytrain, Xtest, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randforest(df,lcols,mask):    \n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    clfForest = RandomForestClassifier()\n",
    "    parameters = {\"n_estimators\": range(1,40)}\n",
    "    # could add njobs to the mix to run them in parallel\n",
    "    clfForest, Xtrain, ytrain, Xtest, ytest = do_classify(clfForest, parameters, df, lcols, u'RESP', 1, mask=mask, score_func='f1')\n",
    "    return clfForest\n",
    "\n",
    "def gradboost(df,lcols,mask):\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    clfGB = GradientBoostingClassifier()\n",
    "    parameters = {\"n_estimators\": range(1, 20)}\n",
    "    clfGB, Xtrain, ytrain, Xtest, ytest = do_classify(clfGB, parameters, df, lcols, u'RESP', 1, mask=mask, score_func='f1')\n",
    "    return clfGB\n",
    "\n",
    "def adaboost(clfForest,df,lcols,mask):\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    clfAda = AdaBoostClassifier(base_estimator=clfForest)\n",
    "    parameters = {\"n_estimators\": range(1, 20)}\n",
    "    clfAda, Xtrain, ytrain, Xtest, ytest = do_classify(clfAda, parameters, df, lcols, u'RESP', 1, mask=mask, score_func='f1')\n",
    "    return clfAda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_features(clfForest,lcols):\n",
    "    importance_list = clfForest.feature_importances_\n",
    "    importance_list, name_list = zip(*sorted(zip(importance_list, lcols)))\n",
    "    plt.barh(range(len(name_list)),importance_list,align='center')\n",
    "    plt.yticks(range(len(name_list)),name_list)\n",
    "    plt.xlabel('Relative Importance in the Random Forest')\n",
    "    plt.ylabel('Features')\n",
    "    plt.title('Relative Importance of Each Feature')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_decision_surface(clf, X_train, Y_train):\n",
    "    plot_step=0.1\n",
    "    \n",
    "    if X_train.shape[1] != 2:\n",
    "        raise ValueError(\"X_train should have exactly 2 columnns!\")\n",
    "    \n",
    "    x_min, x_max = X_train[:, 0].min() - plot_step, X_train[:, 0].max() + plot_step\n",
    "    y_min, y_max = X_train[:, 1].min() - plot_step, X_train[:, 1].max() + plot_step\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step), np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "    clf.fit(X_train,Y_train)\n",
    "    if hasattr(clf, 'predict_proba'):\n",
    "        Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1]\n",
    "    else:\n",
    "        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])    \n",
    "    \n",
    "    Z = Z.reshape(xx.shape)\n",
    "    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Reds)\n",
    "    plt.scatter(X_train[:,0],X_train[:,1],c=Y_train,cmap=plt.cm.Paired)\n",
    "    plt.show()\n",
    "    \n",
    "    print type(X_train[:, 0].min()), type(plot_step)\n",
    "    \n",
    "\n",
    "def plot_rf(clfForest,clflog,lcols,df):\n",
    "    xt = [lcols[e] for e in clfForest.feature_importances_.argsort()[::-1][0:2]]\n",
    "    X_imp = df[xt].values\n",
    "    Y = df.RESP.values\n",
    "\n",
    "    classifiers = [clfForest, clflog]\n",
    "\n",
    "    titleClassifer = ['Random Forest Classifier', \"Logistic Regression Classifier\"]\n",
    "    for c in range(len(classifiers)):\n",
    "        plt.title(titleClassifer[c])\n",
    "        plt.xlabel(xt[0])\n",
    "        plt.ylabel(xt[1])\n",
    "        plot_decision_surface(classifiers[c], X_imp, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(completedf,year):\n",
    "    # use the dataframe for the input year\n",
    "    df = completedf[completedf['year']==year]\n",
    "    \n",
    "    #Split into train and test, create mask\n",
    "    itrain, itest = train_test_split(xrange(df.shape[0]), train_size=0.7)\n",
    "    mask = masking(df,itrain,itest)\n",
    "    \n",
    "    # initialize variables for svm/svc\n",
    "    Cs=[0.001, 0.01, 0.1, 1.0, 10.0, 100.0] # try fewer if doesnt finish up\n",
    "    Xmatrix=df[lcols].values\n",
    "    Yresp=df['RESP'].values\n",
    "    Xmatrix_train=Xmatrix[mask]\n",
    "    Xmatrix_test=Xmatrix[~mask]\n",
    "    Yresp_train=Yresp[mask]\n",
    "    Yresp_test=Yresp[~mask]\n",
    "    \n",
    "    # run svm  \n",
    "    best = runsvm(Xmatrix_train, Yresp_train,Cs)\n",
    "    \n",
    "    # run svc    \n",
    "    best_orig = runsvc(Xmatrix_train, Yresp_train,Cs)\n",
    "    \n",
    "    # run logistic models\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    clflog,_,_,_,_ = do_classify(LogisticRegression(penalty=\"l1\"), {\"C\": [0.001, 0.01, 0.1, 1, 10, 100]}, df, lcols, u'RESP', 1, mask=mask)\n",
    "    clflog2,_,_,_,_ = do_classify(LogisticRegression(penalty=\"l2\"), {\"C\": [0.001, 0.01, 0.1, 1, 10, 100]}, df, lcols, u'RESP', 1, mask=mask)\n",
    "    \n",
    "    #Random forest classifier and make random forest plots\n",
    "    clfForest = randforest(df,lcols,mask)\n",
    "    plot_features(clfForest,lcols)\n",
    "    plot_rf(clfForest,clflog,lcols,df)\n",
    "    \n",
    "    # ADA Boost Classifier\n",
    "    clfAda = adaboost(clfForest,df,lcols,mask)\n",
    "    \n",
    "    # Gradient Boost CLassifier\n",
    "    clfGB = gradboost(df,lcols,mask)\n",
    "\n",
    "    return best, best_orig, clflog, clflog2, clfForest, clfAda, clfGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clfsvm, clfsvc, clflog, clflog2, clfForest, clfAda, clfGB = model(completedf, 2014)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
